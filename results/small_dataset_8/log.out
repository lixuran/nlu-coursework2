INFO: COMMAND: train.py --save-dir /afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/results/small_dataset_8 --log-file /afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/results/small_dataset_8/log.out --data /afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/europarl_prepared --train-on-tiny --arch transformer
INFO: Arguments: {'data': '/afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/europarl_prepared', 'source_lang': 'de', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 10, 'train_on_tiny': True, 'arch': 'transformer', 'max_epoch': 100, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 10, 'log_file': '/afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/results/small_dataset_8/log.out', 'save_dir': '/afs/inf.ed.ac.uk/user/s16/s1603859/PycharmProjects/nlu_cw2/results/small_dataset_8', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 128, 'encoder_ffn_embed_dim': 512, 'encoder_layers': 2, 'encoder_attention_heads': 2, 'decoder_embed_dim': 128, 'decoder_ffn_embed_dim': 512, 'decoder_layers': 2, 'decoder_attention_heads': 2, 'dropout': 0.4, 'attention_dropout': 0.4, 'activation_dropout': 0.4, 'no_scale_embedding': False, 'device_id': 0}
INFO: Loaded a source dictionary (de) with 5047 words
INFO: Loaded a target dictionary (en) with 4420 words
INFO: Built a model with 2707652 parameters
INFO: Epoch 000: loss nan | lr 0.0003 | num_tokens 13.51 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 000: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
